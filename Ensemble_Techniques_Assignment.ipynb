{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. What is an ensemble technique in machine learning?\n",
        "# An ensemble technique is a machine learning method that combines multiple models to improve the overall performance.\n",
        "# It aggregates the predictions of several base models to provide better predictions than any individual model.\n",
        "\n",
        "# Q2. Why are ensemble techniques used in machine learning?\n",
        "# Ensemble techniques are used to:\n",
        "# - Improve accuracy by reducing variance (bagging) or bias (boosting).\n",
        "# - Handle noisy data and imbalanced datasets more effectively.\n",
        "# - Provide more stable and reliable predictions by leveraging multiple models.\n",
        "\n",
        "# Q3. What is bagging?\n",
        "# Bagging (Bootstrap Aggregating) involves training multiple models on different subsets of the training data\n",
        "# sampled with replacement (bootstrapped). The predictions from all models are then aggregated (e.g., voting for classification, averaging for regression).\n",
        "\n",
        "# Q4. What is boosting?\n",
        "# Boosting involves training multiple models sequentially, where each new model tries to correct the errors of the previous model.\n",
        "# The final prediction is made by combining the predictions of all models, usually with a weighted sum.\n",
        "\n",
        "# Q5. What are the benefits of using ensemble techniques?\n",
        "# Benefits of ensemble techniques include:\n",
        "# - Reduced variance and bias.\n",
        "# - Better handling of overfitting and underfitting.\n",
        "# - Improved model accuracy.\n",
        "# - Robustness to noise and outliers.\n",
        "\n",
        "# Q6. Are ensemble techniques always better than individual models?\n",
        "# Not always. While ensemble methods tend to improve performance by combining multiple models,\n",
        "# they are computationally expensive and may not always outperform individual models, especially when those models are already strong.\n",
        "\n",
        "# Q7. How is the confidence interval calculated using bootstrap?\n",
        "# Confidence intervals are calculated by:\n",
        "# 1. Resampling the data with replacement to generate multiple bootstrap samples.\n",
        "# 2. Calculating the statistic of interest (e.g., mean, median) for each sample.\n",
        "# 3. Sorting the statistics and calculating the desired percentile (e.g., 95% confidence interval).\n",
        "\n",
        "# Q8. How does bootstrap work and what are the steps involved in bootstrap?\n",
        "# Steps involved in bootstrap:\n",
        "# 1. Sample with replacement from the original dataset to create bootstrap samples.\n",
        "# 2. Compute the statistic of interest for each bootstrap sample.\n",
        "# 3. Repeat the process multiple times to create a distribution of the statistic.\n",
        "# 4. Calculate the confidence interval by taking the appropriate percentiles of the distribution.\n",
        "\n",
        "# Q9. A researcher wants to estimate the mean height of a population of trees.\n",
        "# The mean height of the sample is 15 meters, the standard deviation is 2 meters,\n",
        "# and the sample size is 50. Use bootstrap to estimate the 95% confidence interval for the population mean height.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Given data\n",
        "sample_mean = 15  # Mean height of the sample\n",
        "sample_std = 2    # Standard deviation of the sample\n",
        "sample_size = 50  # Number of trees in the sample\n",
        "\n",
        "# Generate the sample data (we assume it's normally distributed for simplicity)\n",
        "sample_data = np.random.normal(sample_mean, sample_std, sample_size)\n",
        "\n",
        "# Bootstrap procedure\n",
        "n_iterations = 10000  # Number of bootstrap iterations\n",
        "bootstrap_means = []\n",
        "\n",
        "# Generate bootstrap samples and compute means\n",
        "for i in range(n_iterations):\n",
        "    bootstrap_sample = np.random.choice(sample_data, size=sample_size, replace=True)\n",
        "    bootstrap_means.append(np.mean(bootstrap_sample))\n",
        "\n",
        "# Convert bootstrap means to a numpy array\n",
        "bootstrap_means = np.array(bootstrap_means)\n",
        "\n",
        "# Calculate the 95% confidence interval\n",
        "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
        "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
        "\n",
        "# Output the results\n",
        "print(f\"Bootstrap 95% Confidence Interval for the Mean Height: ({lower_bound}, {upper_bound})\")\n"
      ]
    }
  ]
}